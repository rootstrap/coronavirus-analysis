{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering analysis\n",
    "\n",
    "There are two datasets that have similar information for France and South Korea, respectively. So, the idea is to merge those datasets and see if we can find clusters. \n",
    "\n",
    "You can find the links to the data here: \n",
    "- France: https://www.kaggle.com/lperez/coronavirus-france-dataset\n",
    "- South Korea: https://www.kaggle.com/kimjihoo/coronavirusdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import geopandas\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and check missing values\n",
    "Let's load the data for both datasets and see how many NAs values it has. NAs values corresponds to missing or empty information. Decision should be made for each particular column to prepare the data for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path + '/patient.csv')\n",
    "    df['released_date'] = pd.to_datetime(df['released_date'])\n",
    "    df['confirmed_date'] = pd.to_datetime(df['confirmed_date'])\n",
    "    df['month'] = df['confirmed_date'].dt.month\n",
    "    df['day'] = df['confirmed_date'].dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france = load_data('coronavirusdataset_france')\n",
    "df_france.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_south_korea = load_data('coronavirusdataset_south_korea')\n",
    "df_south_korea.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve missing values\n",
    "\n",
    "- With the objective of keeping the necesary data, some of the columns are removed such as department, comments, health consiedered not important for the analysis. \n",
    "- From the birth date, we can create the age variable, substracting it to the actual date. The missing information will be filled with random numbers draw from a distribution. The information related to the age distribution of the population can be found:     \n",
    " *France:* https://www.indexmundi.com/france/demographics_profile.html  \n",
    " *South Korea:* https://www.indexmundi.com/south_korea/demographics_profile.html\n",
    " \n",
    "So, the 'simulate_age' function is create in order to simulate the population's age based on the available data. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france.drop(['departement','region','comments', 'id', 'infected_by','health','city','source'],axis=1,inplace=True)\n",
    "df_south_korea.drop(['region','disease','patient_id','infected_by'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_age(ranges, percents, total_pop):\n",
    "    simulated_pop = np.array(0)\n",
    "    for (low, high), percent in zip(ranges, percents):\n",
    "        simulated_pop = np.append(simulated_pop, \n",
    "                  np.random.randint(low=low, high=high, size=int(total_pop*percent/100)))\n",
    "    return simulated_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#France\n",
    "france_population = 67364357\n",
    "\n",
    "'''\n",
    "0-14 years: 18.48% \n",
    "15-24 years: 11.8% \n",
    "25-54 years: 37.48% \n",
    "55-64 years: 12.42%\n",
    "65 years and over: 19.82%\n",
    "'''\n",
    "ranges = [(0,14),(15,24),(25,54),(55,64),(65,90)]\n",
    "percents = [18.48,11.8,37.48,12.42,19.82]\n",
    "france_simulated_pop = simulate_age(ranges, percents, france_population)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.hist(france_simulated_pop,bins=20, color='mediumaquamarine', edgecolor='k', alpha=0.5)\n",
    "ax1.set_title('France - Simulated age distribution')\n",
    "\n",
    "#South Korea\n",
    "south_korea_population = 51418097\n",
    "\n",
    "'''\n",
    "0-14 years: 13.03% \n",
    "15-24 years: 12.19%\n",
    "25-54 years: 45.13%\n",
    "55-64 years: 15.09% \n",
    "65 years and over: 14.55% \n",
    "'''\n",
    "percents = [13.03,12.19,45.13,15.09,14.55]\n",
    "south_korea_simulated_pop = simulate_age(ranges, percents, south_korea_population)\n",
    "\n",
    "ax2.hist(south_korea_simulated_pop,bins=20, color='mediumaquamarine', edgecolor='k', alpha=0.5)\n",
    "ax2.set_title('South Korea - Simulated age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the age column, and fill the missing values with a random value choosen from the distributions that we just simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "actual_year = pd.to_datetime('today').year\n",
    "def calculate_age(x):\n",
    "    if math.isnan(x):\n",
    "        return x\n",
    "    else:\n",
    "        return int(actual_year - x)\n",
    "\n",
    "#France\n",
    "df_france['age'] = df_france['birth_year'].apply(calculate_age)\n",
    "df_france.fillna({'age':int(random.choice(france_simulated_pop))}, inplace=True)\n",
    "df_france.drop(['birth_year'], axis=1, inplace=True)\n",
    "\n",
    "#South Korea\n",
    "df_south_korea['age'] = df_south_korea['birth_year'].apply(calculate_age)\n",
    "df_south_korea.fillna({'age':int(random.choice(south_korea_simulated_pop))}, inplace=True)\n",
    "df_south_korea.drop(['birth_year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sex missing values, we can draw a random number with a value of probability based on the sex ratio for each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Considering m as men and w as women. \n",
    "m/w=ratio -> m=ration*w\n",
    "m+w=total_pop\n",
    "-> ratio*w +w=total_pop -> (ratio+1)*w=total_pop -> w=total_pop/(ratio+1)\n",
    "we should divide w by the total in order to get the probability of being women \n",
    "'''\n",
    "def calculate_values(ratio, total_pop):\n",
    "    w = (france_population/(1+ratio))/total_pop\n",
    "    m = 1 - w\n",
    "    return (w,m)\n",
    "\n",
    "\n",
    "# France\n",
    "# total population: 0.96 male(s)/female (2018 est.)\n",
    "w,m = calculate_values(0.96, france_population)\n",
    "#choice among 0 (woman) and 1 (man) with the calculated probabilities\n",
    "df_france['sex'] = df_france['sex'].str.lower()\n",
    "df_france[\"sex\"].replace({\"male\\xa0?\": \"male\"}, inplace=True)\n",
    "df_france.fillna({'sex': np.random.choice(['female','male'],p=[w,m])}, inplace=True)\n",
    "\n",
    "# South Korea\n",
    "# total population: 1 male(s)/female (2018 est.)\n",
    "w,m = calculate_values(1, south_korea_population)\n",
    "df_south_korea['sex'] = df_south_korea['sex'].str.lower()\n",
    "df_south_korea[\"sex\"].replace({\"male\\xa0?\": \"male\"}, inplace=True)\n",
    "df_south_korea.fillna({'sex': np.random.choice(['female','male'],p=[w,m])}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the status column for France's dataset and the state column for South Korea's dataset have the same meaning, we can rename the column for one of the datasets, and update the values to be the same categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france.rename({'status':'state'}, axis=1, inplace=True)\n",
    "df_france['state'] = df_france['state'].apply(lambda x: 'isolated' if (x=='hospital' or x=='home isolation') else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The values for the country variable that are empty will be filled with France or South Korea, respectevily.\n",
    "- A new category 'Unkown' will be created for infection_reason, group, status variables\n",
    "- A new category for infection_order is added with code 0 \n",
    "- The empty values for contact number will be filled with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france.fillna({'country':'France','infection_reason':'Unkown','group':'Unkown', \n",
    "                  'state':'Unknown','infection_order':0, 'contact_number':0} ,\n",
    "                 inplace=True)\n",
    "\n",
    "df_south_korea.fillna({'infection_reason':'Unkown','group':'Unkown', \n",
    "                       'infection_order':0, 'contact_number':0, \n",
    "                       'state':'Unknown'} ,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now which are the missing values that still  need to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_south_korea.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!, we don't have too much left. \n",
    "Now we need to resolve released_date and deceased_date empty values. \n",
    "- If released_date is empty, it means that the person still has the virus. \n",
    "- If deceased_date is empty, it means that the person did not died. \n",
    "\n",
    "So, we can calculate the infection duration in days and remove the other 3 variables.  \n",
    "And transform deceased_date to a binary column, indicated if the person died or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france['released_date'] = df_france[['released_date','deceased_date']].fillna(df_france['deceased_date'])\n",
    "df_france['released_date'] = df_france[['released_date']].fillna(pd.to_datetime('today'))\n",
    "df_france['infection_duration'] = pd.to_datetime(df_france['released_date']).sub(df_france['confirmed_date'], axis=0)\n",
    "df_france = df_france[df_france['infection_duration'].dt.days>=0]\n",
    "df_france['infection_duration'] = df_france['infection_duration'].dt.days\n",
    "df_france.drop(['released_date','confirmed_date','deceased_date'], axis=1, inplace=True)\n",
    "\n",
    "df_south_korea['released_date'] = df_south_korea[['released_date','deceased_date']].fillna(df_south_korea['deceased_date'])\n",
    "df_south_korea['released_date'] = df_south_korea[['released_date']].fillna(pd.to_datetime('today'))\n",
    "df_south_korea['infection_duration'] = pd.to_datetime(df_south_korea['released_date']).sub(df_south_korea['confirmed_date'], axis=0)\n",
    "df_south_korea = df_south_korea[df_south_korea['infection_duration'].dt.days>=0]\n",
    "df_south_korea['infection_duration'] = df_south_korea['infection_duration'].dt.days\n",
    "df_south_korea.drop(['released_date','confirmed_date','deceased_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_france.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_south_korea.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fusion\n",
    "Finally, we are ready to put the two datasets together and start our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_france.append(df_south_korea, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to dummies the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['sex'])], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['country'])], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['state'], drop_first=True)], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['infection_reason'], drop_first=True)], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['group'], drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "Since we have too many variables, it is difficult to find the pattern among the clusters. \n",
    "So, first we can reduce the number of categorical variables by groupping similar categories. \n",
    "After, we can apply a dimension reduction technique to reduce the input variables and make the model easier to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_france.append(df_south_korea, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform infection_reason: let's list the possible values for this variable, and group them. The, transform to dummy variables. Drop one since it is implicit from the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.infection_reason.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_reason(value):\n",
    "    if ('religious' in value or 'parishioner' in value):\n",
    "        return 'religious'\n",
    "    elif ('visit' in value or 'residence' in value):\n",
    "        return 'visit'\n",
    "    elif ('contact' in value):\n",
    "        return 'contact'\n",
    "    elif ('medical' in value or 'health professional' in value):\n",
    "        return 'medical'\n",
    "    elif ('militar' in value):\n",
    "        return 'militar'\n",
    "    elif ('italian' in value):\n",
    "        return 'italian'\n",
    "    elif ('pilgrimage' in value):\n",
    "        return 'pilgrimage'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "df['infection_reason'] = df['infection_reason'].str.lower()\n",
    "df['infection_reason'] = df['infection_reason'].apply(transform_reason)  \n",
    "df = pd.concat([df, pd.get_dummies(df['infection_reason'], prefix='infection_reason', prefix_sep='_')], axis=1)\n",
    "df.drop(['infection_reason_unknown'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 'group' variable provides similar information to infection_reson, it will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['group'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the other categorical variables to dummies: country, state and sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['country'])], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['state'], prefix='state', prefix_sep='_')], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['sex'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['country','state','sex','infection_reason'], axis=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "x = StandardScaler().fit_transform(features.values)\n",
    "pca = PCA(random_state=0)\n",
    "pca.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine number of components with threshold=0.8\n",
    "n_components=np.where(np.cumsum(pca.explained_variance_ratio_)>0.8)[0][0]+1\n",
    "#explained variance\n",
    "v = round(np.cumsum(pca.explained_variance_ratio_)[n_components-1]*100,1)\n",
    "print(f'It is needed {n_components} components to explain {v}% variance of the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components)\n",
    "pcs = pca.fit(x)\n",
    "components_name = list(range(0, n_components))\n",
    "components_name = list(map(lambda x: 'PC' + str(x), components_name))\n",
    "pd.DataFrame(data=pcs.components_, columns = features.columns, index=components_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_range = np.arange(1, n_components+1, 1)\n",
    "components_names = list(map(lambda x: 'PC' + str(x), components_range))\n",
    "plt.matshow(pcs.components_,cmap='viridis')\n",
    "plt.yticks(range(0,n_components), components_names,fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(0,len(features.columns)),features.columns,rotation=90,ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higer values for the variables means more influence in the principal component. Lower values, means negative influence in the principal components. \n",
    "From the matrix, one possible interpretation for the principal componentes is:\n",
    "- PC1: female who is not isolated and is not from from Korea\n",
    "- PC2: first months \n",
    "- PC3: state released\n",
    "- PC4: state deceased\n",
    "- PC5: infection reason religiuos\n",
    "- PC6: low infection duration \n",
    "- PC7: infection reason italian\n",
    "- PC8: infection reason militar\n",
    "- PC9: infection reason medical\n",
    "- PC10: infection reason piligrimage\n",
    "- PC11: high infection order, not from China \n",
    "- PC12: high contact number, not from Mongolia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_test(df, n_init, max_clusters, max_iter):\n",
    "    distortions = []\n",
    "    for i in range(1, max_clusters):\n",
    "        km = KMeans(\n",
    "            n_clusters=i, init='random',\n",
    "            n_init=n_init, max_iter=max_iter,\n",
    "            tol=1e-04, random_state=0\n",
    "        )\n",
    "        km.fit(df)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    plt.plot(range(1, max_clusters), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.show()\n",
    "    \n",
    "    kn = KneeLocator(\n",
    "        range(1, max_clusters),\n",
    "        distortions,\n",
    "        curve='convex',\n",
    "        direction='decreasing',\n",
    "        interp_method='interp1d',\n",
    "    )\n",
    "    return kn.knee\n",
    "\n",
    "def plot_countries(df, title, country_column_name, column_name, n_clusters, legend=False):\n",
    "    world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "    merged_inner = pd.merge(left=world, right=df, left_on='name',right_on=country_column_name, how='inner')\n",
    "    cmap = cm.get_cmap('Spectral', n_clusters)\n",
    "    ax = world.plot(figsize=(20,5), linewidth=0.25, edgecolor='white', color='lightgrey')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    merged_inner.plot(column=column_name, ax=ax, cmap=cmap, legend=legend)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(df, col_1, col_2, cluster_column, num_clusters, title):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(col_1)\n",
    "    ax.set_ylabel(col_2)\n",
    "    labels = list(range(0,num_clusters))\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, num_clusters))\n",
    "    axs = []\n",
    "    for i in labels:\n",
    "        axs.append(ax.scatter(df[df[cluster_column]==i][col_1], df[df[cluster_column]==i][col_2], cmap=colors[i]))\n",
    "    \n",
    "    ax.legend(axs, labels, loc='center', bbox_to_anchor=(0.92, 0.84), ncol=1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_scatter(df, col_1, col_2, col_3, cluster_column, num_clusters, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(col_1)\n",
    "    ax.set_ylabel(col_2)\n",
    "    ax.set_zlabel(col_3, rotation=90)\n",
    "    labels = list(range(0,num_clusters))\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, num_clusters))\n",
    "    axs = []\n",
    "    for i in labels:\n",
    "        d = df[df[cluster_column]==i]\n",
    "        axs.append(ax.scatter(d[col_1], d[col_2], d[col_3], cmap=colors[i]))\n",
    "    \n",
    "    ax.legend(axs, labels, bbox_to_anchor=(0.2, 0.5), ncol=1)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a dataframe from the principal components scores and use that for the clustering analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(data = pca.fit_transform(x), columns = components_names)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the elbow test in order to define the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = elbow_test(pca_df, 10, 20, 300)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "y = km.fit_predict(pca_df)\n",
    "idx = np.argsort(km.cluster_centers_.sum(axis=1))\n",
    "lut = np.zeros_like(idx)\n",
    "lut[idx] = np.arange(n_clusters)\n",
    "pca_df['cluster'] = lut[km.labels_]\n",
    "df['cluster'] = lut[km.labels_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that clusters 5 has only 2 elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_df[pca_df['cluster']==5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the PC7 value is high. It corresponds to the infection reason 'italian'. We can corroborate this by looking at the actual data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_3d_scatter(pca_df, 'PC1', 'PC2', 'PC3', 'cluster', n_clusters, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that some of the clusters are distributed according the first 3 principal components.  \n",
    "However, In this graph, cluster 3 and 4 are not well defined. \n",
    "\n",
    "| Cluster | PC1  |  PC2  | PC3  |\n",
    "| :------:|:----:|:-----:|:----:|\n",
    "| 0       | low  |  low  | low  |\n",
    "| 1       | low  |middle | low  |\n",
    "| 2       | high |middle | low  |\n",
    "\n",
    "The meaning for the principal components was defined as follows: \n",
    "- PC1: female who is not isolated and is not from from Korea\n",
    "- PC2: first months \n",
    "- PC3: state released   \n",
    "\n",
    "So, we can conclude:    \n",
    "\n",
    "| Cluster | Meaning                                                   |\n",
    "|:-------:|:----------------------------------------------------------|\n",
    "|    0    | mostly men from Korea not released in the thirth month    |\n",
    "|    1    | mostly men from Korea not released in the first two months|\n",
    "|    2    | mostly female from France not released                    |    \n",
    "\n",
    "   \n",
    "So, let's graph other components, to see if we can find the meaning of clusters 3 and 4. For this we can draw a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_scatter(pca_df, 'PC3', 'PC5', 'cluster', n_clusters, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grap we can see that PC5 values for Cluster 3 are high. It means that corresponds to infection reason religious. We can check that with the actual data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster']==3].infection_reason.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no clear cluster definition for cluster 4.  \n",
    "\n",
    "## Conclusion \n",
    "\n",
    "In summary, these are the clusters that we found:\n",
    "\n",
    "\n",
    "| Cluster | Meaning                                                   |\n",
    "|:-------:|:----------------------------------------------------------|\n",
    "|    0    | mostly men from Korea not released in the thirth month    |\n",
    "|    1    | mostly men from Korea not released in the first two months|\n",
    "|    2    | mostly female from France not released                    |\n",
    "|    3    | infection reason religious                                |\n",
    "|    4    | others                                                    |\n",
    "|    5    | infection reason italian                                  |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
